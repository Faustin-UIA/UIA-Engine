name: UIA Bench (Gemini)

on:
  workflow_dispatch:
    inputs:
      concurrency:
        description: 'Concurrency level (1 or 2 recommended for stability)'
        required: false
        default: '1'
      gemini_version:
        description: 'Gemini model (e.g., gemini-2.5-flash, gemini-2.5-pro, gemini-2.5-pro-vision)'
        required: false
        default: 'gemini-2.5-flash'

permissions:
  contents: write

concurrency:
  group: uia-bench-gemini-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install dependencies (Strict & Reproducible)
        run: |
          set -e
          # Utiliser 'npm ci' pour une installation propre bas√©e sur package-lock.json.
          # Si le package @google/generative-ai est dans les d√©pendances, 'npm ci' suffit.
          # Si ce n'est pas le cas, ajoutez-le aux 'dependencies' de votre package.json.
          npm ci
          # ‚ùå ANCIEN : npm i -E @google/generative-ai (Redondant et peut casser la reproductibilit√© de 'npm ci')
          
          # V√©rification de l'installation
          npm list @google/generative-ai
          echo "‚úì Dependencies installed successfully"

      - name: Validate Setup and Environment
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          set -e
          
          echo "üîç Environment Validation"
          echo "=========================="
          
          # Verify Node version
          echo "‚úì Node version: $(node --version)"
          echo "‚úì NPM version: $(npm --version)"
          
          # Verify Google Generative AI SDK
          if ! npm list @google/generative-ai > /dev/null 2>&1; then
            echo "‚ùå Google Generative AI SDK (@google/generative-ai) not installed properly"
            npm list @google/generative-ai
            exit 1
          fi
          echo "‚úì Google Generative AI SDK @google/generative-ai installed"
          
          # Verify API key
          if [ -z "$GEMINI_API_KEY" ]; then
            echo "‚ùå GEMINI_API_KEY secret is not set"
            echo "   Configure in: GitHub Settings > Secrets and variables > Actions > GEMINI_API_KEY"
            exit 1
          fi
          echo "‚úì GEMINI_API_KEY is set"
          
          # Verify index.js exists
          if [ ! -f "index.js" ]; then
            echo "‚ùå index.js not found in repository root"
            exit 1
          fi
          echo "‚úì index.js found"
          
          # Check if Gemini provider is implemented
          if ! grep -q "PROVIDER: gemini" index.js; then
            echo "‚ö†Ô∏è  WARNING: PROVIDER: gemini not found in index.js"
            echo "   Make sure callLLM_Gemini() is integrated into callLLM() function"
            echo "   Workflow will still attempt to run..."
          fi
          
          echo ""
          echo "‚úì All environment checks passed"

      - name: Run UIA Bench (Gemini) with Retry Logic
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          PROVIDER: gemini
          NODE_OPTIONS: --max-old-space-size=4096
          LOG_LEVEL: debug
        run: |
          set -euo pipefail
          
          mkdir -p results
          TS=$(date -u +"%Y-%m-%dT%H%M%SZ")
          LOG="results/uia_gemini_${TS}.jsonl"
          
          ATTEMPT=0
          MAX_ATTEMPTS=3
          CONCURRENCY="${{ github.event.inputs.concurrency || '1' }}"
          MODEL="${{ github.event.inputs.gemini_version || 'gemini-2.5-flash' }}"
          
          echo ""
          echo "üöÄ Starting UIA Bench (Gemini)"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "Model:       ${MODEL}"
          echo "Concurrency: ${CONCURRENCY} (low for stability)"
          echo "Max retries: ${MAX_ATTEMPTS}"
          echo "Log file:    ${LOG}"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo ""
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            echo "Attempt ${ATTEMPT}/${MAX_ATTEMPTS}"
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            
            # Utilisation de 'node index.js' et non 'node .' pour √™tre explicite
            if node index.js \
              --A=all --prompts=all --concurrency="${CONCURRENCY}" \
              --model="${MODEL}" --t=0.2 --max_tokens=180 \
              --metrics=true --diag=true --log="${LOG}"; then
              
              echo ""
              echo "‚úì Run completed successfully on attempt ${ATTEMPT}"
              break
              
            else
              EXIT_CODE=$?
              
              if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
                echo ""
                echo "‚ö†Ô∏è Attempt ${ATTEMPT} failed (exit code: ${EXIT_CODE})"
                echo "Waiting 60 seconds before retry..."
                sleep 60
              else
                echo ""
                echo "‚ùå All ${MAX_ATTEMPTS} attempts failed"
                exit $EXIT_CODE
              fi
            fi
          done
          
          echo "LOG_PATH=${LOG}" >> "$GITHUB_ENV"

      - name: Validate Run Quality
        if: always()
        run: |
          python3 << 'VALIDATE_EOF'
          import json
          import sys
          import os
          
          log_path = os.environ.get('LOG_PATH', 'results/latest.jsonl')
          
          if not os.path.exists(log_path):
              print(f"‚ùå Log file not found: {log_path}")
              sys.exit(1)
          
          # Parse JSONL
          events = []
          try:
              with open(log_path) as f:
                  for line in f:
                      if line.strip():
                          try:
                              events.append(json.loads(line))
                          except json.JSONDecodeError:
                              pass
          except Exception as e:
              print(f"‚ùå Error reading log file: {e}")
              sys.exit(1)
          
          # Count metrics
          valid_responses = 0
          empty_responses = 0
          prompt_results = 0
          streaming_true = 0
          latencies = []
          entropies = []
          EMPTY_SHA = 'da39a3ee5e6b'
          
          for e in events:
              if e.get('event') == 'PROMPT_RESULT':
                  prompt_results += 1
                  ms = e.get('output_ms', 0)
                  sha = e.get('output_text_sha')
                  
                  # Ajout d'une v√©rification pour s'assurer que les m√©triques ne sont pas nulles, 
                  # car index.js peut renvoyer { metrics: null } si ARG_METRICS est faux,
                  # mais ici ARG_METRICS=true.
                  if e.get('metrics') is not None and ms > 0 and sha != EMPTY_SHA:
                      valid_responses += 1
                      latencies.append(ms)
                      
                      # Extract entropy
                      ent = e.get('metrics', {}).get('entropy', {}).get('mean_H', 0)
                      if ent > 0:
                          entropies.append(ent)
                  else:
                      empty_responses += 1
              
              # L'√©v√©nement STREAM_SUMMARY semble √™tre remplac√© par les donn√©es dans PROMPT_RESULT (phases)
              # La v√©rification de 'streaming_true' est moins critique ici si les phases sont utilis√©es.
              if e.get('phases', {}).get('streaming'):
                  streaming_true += 1


          
          # Calculate metrics
          valid_rate = 100 * valid_responses / prompt_results if prompt_results > 0 else 0
          # Correction du calcul du taux de streaming, bas√© sur les r√©sultats de prompt valides
          streaming_rate = 100 * streaming_true / valid_responses if valid_responses > 0 else 0 
          
          print("üìä RUN QUALITY VALIDATION")
          print("=" * 70)
          print(f"Total PROMPT_RESULT events:    {prompt_results}")
          print(f"Valid responses:               {valid_responses} ({valid_rate:.1f}%)")
          print(f"Empty responses:               {empty_responses} ({100-valid_rate:.1f}%)")
          print(f"Streaming responses:           {streaming_true} ({streaming_rate:.1f}%)")
          
          if latencies:
              sorted_lat = sorted(latencies)
              lat_len = len(sorted_lat)
              print(f"\n‚è±Ô∏è  LATENCY DISTRIBUTION (Valid Responses Only)")
              print(f"   Min:  {min(latencies):7.0f} ms")
              # Calculs de Px5 robustes
              print(f"   P25:  {sorted_lat[int(0.25*lat_len)]:7.0f} ms")
              print(f"   P50:  {sorted_lat[int(0.50*lat_len)]:7.0f} ms")
              print(f"   P75:  {sorted_lat[int(0.75*lat_len)]:7.0f} ms")
              print(f"   P95:  {sorted_lat[int(0.95*lat_len)]:7.0f} ms")
              print(f"   Max:  {max(latencies):7.0f} ms")
              print(f"   Avg:  {sum(latencies)/lat_len:7.0f} ms")
          
          if entropies:
              ent_len = len(entropies)
              print(f"\nüìà ENTROPY DISTRIBUTION (Valid Responses Only)")
              sorted_ent = sorted(entropies)
              print(f"   Min:  {min(entropies):6.3f}")
              print(f"   P25:  {sorted_ent[int(0.25*ent_len)]:6.3f}")
              print(f"   P50:  {sorted_ent[int(0.50*ent_len)]:6.3f}")
              print(f"   P75:  {sorted_ent[int(0.75*ent_len)]:6.3f}")
              print(f"   Max:  {max(entropies):6.3f}")
              print(f"   Avg:  {sum(entropies)/ent_len:6.3f}")
          
          print("\n" + "=" * 70)
          
          MIN_VALID_RATE = 90.0
          
          if valid_rate < MIN_VALID_RATE:
              print(f"‚ùå FAIL: Valid rate {valid_rate:.1f}% < required {MIN_VALID_RATE}%")
              print(f"\nüí° This run is NOT suitable for CSI fusion analysis.")
              print(f"   Root cause: Likely Gemini API timeouts/rate-limiting")
              print(f"   Recommendation: Reduce concurrency or retry later")
              sys.exit(1)
          else:
              print(f"‚úì PASS: Valid rate {valid_rate:.1f}% >= required {MIN_VALID_RATE}%")
              print(f"\n‚úì This run IS suitable for CSI fusion analysis")
              sys.exit(0)
          
          VALIDATE_EOF

      - name: Upload results
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: uia-gemini-results-${{ github.run_id }}
          path: ${{ env.LOG_PATH }}
          retention-days: 30
          if-no-files-found: error

      - name: Create Detailed Summary
        if: always()
        run: |
          MODEL="${{ github.event.inputs.gemini_version || 'gemini-2.5-flash' }}"
          CONCURRENCY="${{ github.event.inputs.concurrency || '1' }}"
          LOG_FILE="${{ env.LOG_PATH }}"
          
          echo "## üî¨ UIA Bench (Gemini) - Run Report" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### ‚úì Run Information" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Model**: \`${MODEL}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Concurrency**: \`${CONCURRENCY}\` (low for stability)" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Timestamp**: $(date -u +"%Y-%m-%dT%H:%M:%SZ")" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Workflow**: [Run #${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          
          if [ -n "${LOG_FILE}" ] && [ -f "${LOG_FILE}" ]; then
            LOG_SIZE=$(wc -c < "${LOG_FILE}")
            LOG_LINES=$(wc -l < "${LOG_FILE}")
            echo "### üìã Artifact Details" >> "$GITHUB_STEP_SUMMARY"
            echo "- **File**: \`${LOG_FILE}\`" >> "$GITHUB_STEP_SUMMARY"
            echo "- **Size**: $(numfmt --to=iec-i --suffix=B ${LOG_SIZE} 2>/dev/null || echo '${LOG_SIZE} bytes')" >> "$GITHUB_STEP_SUMMARY"
            echo "- **Lines**: ${LOG_LINES}" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi
          
          echo "### üì¶ Artifact Download" >> "$GITHUB_STEP_SUMMARY"
          echo "1. Download: **uia-gemini-results-${{ github.run_id }}**" >> "$GITHUB_STEP_SUMMARY"
          echo "2. Verify: Check JSONL format and response counts" >> "$GITHUB_STEP_SUMMARY"
          echo "3. Analyze: \`python uia_fusion_analyzer.py <log.jsonl>\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          
          echo "### üìö CSI Fusion Analysis" >> "$GITHUB_STEP_SUMMARY"
          echo "Once validated, use the fusion analyzer to generate:" >> "$GITHUB_STEP_SUMMARY"
          echo "- \`*_fusion_summary.json\` - Top AX/BY scores + confidence" >> "$GITHUB_STEP_SUMMARY"
          echo "- \`*_fusion_perA.csv\` - Per-A-code CSI medians" >> "$GITHUB_STEP_SUMMARY"
          echo "- \`*_validators_perA.csv\` - Validator board pass/fail counts" >> "$GITHUB_STEP_SUMMARY"

      - name: Notify on Validation Failure
        if: failure()
        run: |
          echo "‚ùå UIA Bench (Gemini) - VALIDATION FAILED" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Possible Causes" >> "$GITHUB_STEP_SUMMARY"
          echo "1. **npm install failed**: Check the 'Install dependencies' step logs" >> "$GITHUB_STEP_SUMMARY"
          echo "2. **Gemini SDK Not Implemented**: The integration in index.js failed. Check the 'Run UIA Bench' step logs." >> "$GITHUB_STEP_SUMMARY"
          echo "3. **API Key Issue**: Check GEMINI_API_KEY secret in Settings > Secrets" >> "$GITHUB_STEP_SUMMARY"
          echo "4. **Gemini API Timeouts**: SDK couldn't fetch responses in time" >> "$GITHUB_STEP_SUMMARY"
          echo "5. **Rate Limiting**: Too many concurrent requests" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Recommended Actions" >> "$GITHUB_STEP_SUMMARY"
          echo "- ‚úì Check 'Install dependencies' step for npm errors" >> "$GITHUB_STEP_SUMMARY"
          echo "- ‚úì Verify the \`callLLM_Gemini\` function in **index.js** (corrected in the previous response)" >> "$GITHUB_STEP_SUMMARY"
          echo "- ‚úì Check GitHub Actions logs for detailed error messages" >> "$GITHUB_STEP_SUMMARY"
          echo "- ‚úì Verify GEMINI_API_KEY secret is configured correctly" >> "$GITHUB_STEP_SUMMARY"
          echo "- ‚úì Try reducing concurrency further: \`--concurrency=1\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- ‚úì Consider switching to OpenAI (more stable)" >> "$GITHUB_STEP_SUMMARY"